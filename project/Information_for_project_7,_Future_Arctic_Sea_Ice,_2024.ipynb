{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HblcszRqQn5i"
   },
   "source": [
    "# Information for groups working on project 7: Future Arctic sea ice in CMIP6 models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKSVQynlQn5m"
   },
   "source": [
    "The aim of this notebook is to give you some information that might be useful for starting your project:\n",
    "\n",
    "- Part A provides you with a quick introduction to the CMIP6 project and ensemble modelling. It also contains a list of links that provide details about the data that are available to you in the CMIP6 catalogue. You can use this to help you to decide what data you want to analyse for your project.\n",
    "- Part B provides some examples of how to plot data on maps easily, by combining xarray (which you saw in Guillaume's Pangeo session) with Cartopy\n",
    "- Part C gives you more specific information about this project, about sea ice, and about what you should be aiming to do. You will need to apply clustering techniques to sea ice, and perhaps other related variables, in order to characterise the Arctic sea ice evolution in different models.\n",
    "\n",
    "Note: the installation of the libraries to plot maps using Cartopy can take a few minutes. You might want to execute the first code cell of part B now, before you read the information below, so that you don't have to wait later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6xOeJOBQn5o"
   },
   "source": [
    "## A. Data sources and information about CMIP6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV0caRt2Qn5p"
   },
   "source": [
    "### 1. Introduction\n",
    "Project 7 requires you to analyse model output from the CMIP6 project. It is possible to access these data a number of ways, including via the cloud, since Google host a part of these data via their Public Datasets programme (https://cloud.google.com/blog/products/data-analytics/new-climate-model-data-now-google-public-datasets). For your project, this is how you will access the CMIP6 data. As you have seen in class, this method means that you will be able to analyse the data directly in Colab without downloading anything to your machine.\n",
    "\n",
    "### 2. What is ensemble modelling and why do we do it?\n",
    "CMIP6 uses an ensemble modelling strategy. What does this mean?\n",
    "\n",
    "In the context of CMIP6, an ensemble of model simulations is a collection of simulations that have been created using exactly the same configuration for everything, with the exception of the initial conditions. These small differences in the initial state can lead to much larger differences in the forecast over time, as popularised in the so-called \"butterfly effect\".\n",
    "\n",
    "We use ensemble modelling to help us understand the probability of the events simulated by the model occurring. For example, imagine that we use an ensemble of 5 simulations to try to forecast next week's weather, and all 5 simulations predict that it will rain in Brest. In this case, we will be more likely to believe that it will rain next week than if our results had been that 2 of our simulations told us that it would rain, 2 told us that it would be sunny and 1 told us that it would snow.\n",
    "\n",
    "The aim of the ensemble approach in climate modelling (as in CMIP6) is the same as in the weather forecasting case: it is an attempt to quantify how much we believe the predictions of the future climate.\n",
    "\n",
    "### 3. What is CMIP6?\n",
    "CMIP = Coupled Model Intercomparison Project (wikipedia page: https://en.wikipedia.org/wiki/Coupled_Model_Intercomparison_Project)\n",
    "\n",
    "The CMIP programme aims to improve our knowledge of climate change. It began in 1995, and has already been through 5 \"phases\". Each \"phase\" corresponds to a coordinated effort to collect together a number of climate simulations created by research centres around the world. These simulations are then made freely available for anybody to download and analyse. Phase 6 is now underway: the data that you will be analysing are thus only recently available, and are likely to be used in research studies for about the next ten years or so.\n",
    "\n",
    "A number of experiments are defined at each phase. These often consist of \"historical\" experiments, which simulate the climate over the past decade, and a number of future experiments, which simulate the future climate in response to certain prescribed greenhouse gas concentration scenarios, which should represent the response to possible future decisions made by society. (If you are interested in reading more about these, this article provides a nice introduction: https://www.carbonbrief.org/explainer-how-shared-socioeconomic-pathways-explore-future-climate-change)\n",
    "\n",
    "### 4. What data are available for your projects?\n",
    "The CMIP6 catalogue contains a lot of data: the entire archive comprises about 20PB of outputs! Not all available data is stored in the Cloud, so you will need to check to see what is available when deciding which model(s) and scenario(s) you want to analyse.\n",
    "\n",
    "The people at Pangeo have written some useful summaries that describe:\n",
    "\n",
    "- the data available in the Cloud: https://docs.google.com/document/d/1yUx6jr9EdedCOLd--CPdTfGDwEwzPpCF6p1jRmqx-0Q/edit\n",
    "- a list of the models available: https://docs.google.com/spreadsheets/d/13DHeTEH_8G08vxTMX1Fs-WbAA6SamBjDdh0fextdcGE/edit#gid=165882553\n",
    "- a list of the variables available: https://docs.google.com/spreadsheets/d/1UUtoz6Ofyjlpx5LdqhKcwHFz2SGoTQV2_yekHyMfL9Y/edit#gid=1221485271\n",
    "\n",
    "They have also produced an example notebook that shows how to load in air temperature. You can either run it via Binder on the Pangeo infrastructure: https://binder.pangeo.io/v2/gh/pangeo-data/pangeo-cmip6-examples/master or run it directly in Colab: https://colab.research.google.com/drive/19iEVxE_9QoTeg4st7MmucHJUmO93NXHp\n",
    "\n",
    "Their notebook is an excellent introduction: it should provide you with everything that you need to get started loading the data. I recommend that you look at it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzgnr4wUQn5s"
   },
   "source": [
    "## B. Using xarray with Cartopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pz1V6fbuQn5s"
   },
   "source": [
    "Last week we talked a lot about accessing and analysing data, but we didn't talk much about how to make figures. In the geosciences, often we want to make maps. Unfortunately, sometimes this can be more difficult than we would like! Model grids are often irregular, and it can be difficult to plot data in the polar regions, where all longitudes get closer and closer together, and finally converge to a single point.\n",
    "\n",
    "The Pangeo notebook shows an example of loading CMIP6 data using xarray. One of the really nice features of xarray is that it integrates with the plotting library Cartopy (https://scitools.org.uk/cartopy/docs/latest/) to enable us to change map projections and easily transform our data. We'll look at an example for the Arctic below.\n",
    "\n",
    "Let's set up the libraries that we will need, and then load in the sea surface height for the IPSL model in the Shared Socioeconomic Pathway 2 scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_oEFQiWfcyU6",
    "outputId": "ed711a71-0554-4d30-8696-0dc4884afaff"
   },
   "outputs": [],
   "source": [
    "# first we have to install some libraries in Colab\n",
    "!pip install zarr\n",
    "!pip install gcsfs\n",
    "\n",
    "# and set up Cartopy\n",
    "!pip uninstall shapely --yes\n",
    "\n",
    "# note: the next step produces an error in pip. You can ignore this error.\n",
    "!pip install lida==0.0.10 shapely cartopy --no-binary shapely --no-binary cartopy --use-deprecated=legacy-resolver\n",
    "\n",
    "# and manually download shape files for Cartopy\n",
    "!wget https://raw.githubusercontent.com/SciTools/cartopy/main/lib/cartopy/feature/download/__main__.py -O cartopy_feature_download.py\n",
    "!python cartopy_feature_download.py physical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c17xuPwmQn5x"
   },
   "outputs": [],
   "source": [
    "# set up our libraries\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import zarr\n",
    "import gcsfs\n",
    "\n",
    "# set up cartopy\n",
    "import cartopy.crs as ccrs\n",
    "from matplotlib.axes import Axes\n",
    "from cartopy.mpl.geoaxes import GeoAxes\n",
    "GeoAxes._pcolormesh_patched = Axes.pcolormesh\n",
    "\n",
    "# notebook display options\n",
    "xr.set_options(display_style='html')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQApo9UMKGmV"
   },
   "source": [
    "Now let's look at the available data in the catalogue, and select one of the simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HUQfuejyQn5x",
    "outputId": "59427620-feec-4c4f-e718-f828491d3f17"
   },
   "outputs": [],
   "source": [
    "# read in the catalogue information\n",
    "df = pd.read_csv('https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv')\n",
    "\n",
    "# look for simulations that correspond to our chosen criteria\n",
    "df_ssh = df.query(\"activity_id=='ScenarioMIP' & table_id == 'Omon' & variable_id == 'zos' & experiment_id == 'ssp245' & institution_id == 'IPSL'\")\n",
    "\n",
    "# print out the metadata for the available simulations in the catalogue\n",
    "df_ssh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HI-228mKtuC"
   },
   "source": [
    "The table shows multiple available outputs for the same forcing scenario and model: these are the different members (member_id column) of the ensemble for that model and scenario. Now let's select one of the members:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJCCh3q1Kt8J"
   },
   "outputs": [],
   "source": [
    "# setup the gcs access\n",
    "gcs = gcsfs.GCSFileSystem(token='anon')\n",
    "\n",
    "# get the path to a specific zarr store (the last one from the dataframe above)\n",
    "zstore = df_ssh.zstore.values[-1]\n",
    "\n",
    "# create a mapping interface to the store\n",
    "mapper = gcs.get_mapper(zstore)\n",
    "\n",
    "# open it using xarray and zarr\n",
    "ds = xr.open_zarr(mapper, consolidated=True,decode_times=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyZ7oKEaB5KR"
   },
   "source": [
    "The name given to the latitude and longitude variables changes depending on the model, so let's check the names of the coordinate variables for this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_JqDZ9ukBOj7",
    "outputId": "3484f638-efe9-4ff3-e68d-be613cfc6d33"
   },
   "outputs": [],
   "source": [
    "ds.coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETh5ilVdQn5z"
   },
   "source": [
    "Now we'll select only the Arctic region: we'll choose latitude > 55°N:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 823
    },
    "id": "g-aPzs6zQn50",
    "outputId": "24d157b6-1999-433c-aa38-3ec9bb052298"
   },
   "outputs": [],
   "source": [
    "lat = ds.nav_lat.compute()\n",
    "ds=ds.where(lat > 55,drop=True)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5vGi0dWQn53"
   },
   "source": [
    "Now let's see what happens if we try to plot a map for a specific date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "VaBP9mClQn54",
    "outputId": "23afd252-21a7-4ee4-f2fd-8b2a4c49b1be"
   },
   "outputs": [],
   "source": [
    "ds.zos.sel(time=ds.time[-1]).squeeze().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nM82SVoJQn54"
   },
   "source": [
    "It's very difficult to see what's going on in the Arctic with this projection. We'll manage this by using the Polar Stereographic projection, which has the North Pole in the middle of the figure, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "-uaFaIFOQn54",
    "outputId": "56293497-4be6-46b5-feb5-696f2d4d6af5"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection=ccrs.NorthPolarStereo())\n",
    "ax.set_extent([-180, 180, 55, 90], ccrs.PlateCarree())\n",
    "ax.coastlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvDMdkPoQn55"
   },
   "source": [
    "To be able to plot our data on this map, we also need to apply a transformation. We can just tell xarray to do this for us, by telling it the name of the transformation to apply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "id": "bSa8M6sZQn55",
    "outputId": "a8a47e90-aa0f-4fef-c0a3-74e690554e14"
   },
   "outputs": [],
   "source": [
    "# first we set up our axes with the projection that we want\n",
    "ax = plt.axes(projection=ccrs.NorthPolarStereo());\n",
    "# then we set the latitude limits on our figure (here, we include all longitudes\n",
    "# (-180° -> 180°) and 55°N -> 90°N)\n",
    "ax.set_extent([-180, 180, 55, 90], ccrs.PlateCarree())\n",
    "# now we tell xarray to plot our data. We pass a number of extra parameters:\n",
    "# transform=ccrs.PlateCarree() -> tells xarray how to transform the data\n",
    "# x='nav_lon' -> tells xarray the name of the longitude variable\n",
    "# y='nav_lat' -> tells xarray the name of the latitude variable\n",
    "ds.zos[0].plot.pcolormesh(ax=ax,transform=ccrs.PlateCarree(),x='nav_lon', y='nav_lat', add_colorbar=True);\n",
    "# finally, we add the coastlines\n",
    "ax.coastlines();\n",
    "# and add grid lines\n",
    "ax.gridlines(draw_labels=True, x_inline=False, y_inline=True,color='grey');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXPYAESiaaD0"
   },
   "source": [
    "## C. Aims for your project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4M0g95NpabhH"
   },
   "source": [
    "In this project you will aim to describe the evolution of the Arctic sea ice according to different models and/or radiative forcing scenarios. Arctic sea ice decrease has not been uniform over time: certain regions are losing ice much more quickly than others, and this is expected to continue into the future.\n",
    "\n",
    "Sea ice plays a very important role in the climate system for many reasons; one of the most important is that it acts as a very effective insulating \"lid\" on the ocean. The ocean is warm relative to the atmospheric temperature in the Arctic, and so if the sea ice disappears, this can lead to huge changes in the quantity of heat exchanged between the atmosphere and ocean. Depending on the location at which these changes in heat transfer occur, the impact on the atmosphere and its large-scale circulation can be very different, and so it is important to understand not only how much ice is being lost, but also where the ice is being lost. Your aim in this project will be to explore these spatial differences between different models, variables, and emission scenarios.\n",
    "\n",
    "The Arctic is divided into a number of regional seas, and sea ice change is often calculated as an average within each of these regions:\n",
    "<img src='https://www.ncei.noaa.gov/monitoring-content/snow-and-ice/regional-sea-ice/images/region_definitions.png' width=\"500\">\n",
    "\n",
    "However, these regions are just geographical, they are not defined based on physical criteria. Your aim will be to discover how appropriate these regional definitions are for describing the future Arctic sea ice in the CMIP6 models.\n",
    "\n",
    "Your main task in this project will be to apply clustering techniques to the time series of sea ice concentration, with the aim of identifying coherent regions of sea ice loss. You can also treat sea ice volume if you have time. You might compare the results obtained using multiple emissions scenarios to look at the consistency of the regional patterns.\n",
    "\n",
    "### Choosing the variables to use\n",
    "In the CMIP6 model outputs, you will find that a number of different variables relating to sea ice are available. The most widely available one is **\"sea-ice area percentage\"**: this is normally known as \"sea ice concentration\", and represents the percentage of each grid cell that is covered by sea ice.\n",
    "\n",
    "Sea ice is not a 2D quantity, it also has a height, and so to better understand the processes that act on sea ice, we also use **sea ice volume**. This is not normally supplied directly by models, but can be calculated by multipling **sea ice thickness * sea ice concentration * grid-cell area for ocean variables**. You will need to retrieve these 3 variables and calculate this quantity yourselves.\n",
    "\n",
    "Although other sea ice variables are produced by a number of models, the consistency of their availability across the different models and scenarios is not so good. Because of this, I recommend that you use only the sea ice concentration and sea ice volume variables to characterise the sea ice.\n",
    "\n",
    "### Time averaging\n",
    "**Seasonality**: it is possible that the patterns of sea ice loss will depend on the season, and so you should analyse each season separately. I recommend that you average the data as follows for each year:\n",
    "* winter = December, January, February\n",
    "* spring = March, April, May\n",
    "* summer = June, July, August\n",
    "* autumn = September, October, November\n",
    "\n",
    "xarray can perform this averaging for you with the ```.resample``` method. An example is shown below to help you get started. It also shows you how to resample the data to have a 5-year mean for each season: this 5-year averaging is not obligatory, but it might be helpful to make this average in order to keep the data size manageable.\n",
    "\n",
    "Once you have calculated these seasonal averages, you will then perform a separate clustering analysis for each season.\n",
    "\n",
    "**To make the tasks simpler, you can restrict your analysis to the central Arctic region, north of 70°N**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 861
    },
    "id": "ZuD7VIPu5cIY",
    "outputId": "a9904dea-e0b6-4235-d4a8-7c20439359e3"
   },
   "outputs": [],
   "source": [
    "# example of accessing the sea ice concentration data and calculating\n",
    "# the seasonal averages using xarray:\n",
    "\n",
    "# first we select a model\n",
    "df_tk = df.query(\"activity_id=='ScenarioMIP' & table_id == 'SImon' & variable_id == 'siconc' & experiment_id == 'ssp245' & institution_id == 'IPSL'\")\n",
    "gcs = gcsfs.GCSFileSystem(token='anon')\n",
    "zstore = df_tk.zstore.values[0]\n",
    "\n",
    "# now we load the data into an xarray data set:\n",
    "mapper = gcs.get_mapper(zstore)\n",
    "ds_sic = xr.open_zarr(mapper, consolidated=True)\n",
    "\n",
    "# we'll restrict this to the region north of 70°N again:\n",
    "lat = ds_sic['nav_lat'].compute() # we use .compute() here because we need to\n",
    "# actually load this data so that we can use it to index the other variables:\n",
    "# xarray is naturally \"lazy\", and will not load the data until it is obliged to\n",
    "ds_sic = ds_sic.where(lat > 70, drop=True)\n",
    "\n",
    "# print out the metadata for this simulation: at this point we have all available\n",
    "# time steps: 1 per month\n",
    "ds_sic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PdpQcJyQf46O",
    "outputId": "41a41549-ed8e-4eb7-80cc-475efa3aefa1"
   },
   "outputs": [],
   "source": [
    "print('The simulation contains {} years of monthly data, between {} and {}'.format(ds_sic.time.dt.year.max().data-ds_sic.time.dt.year.min().data,ds_sic.time.dt.year.min().data, ds_sic.time.dt.year.max().data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BCn8bDZed-s7"
   },
   "outputs": [],
   "source": [
    "# now let's make the seasonal averages, we do this using resample:\n",
    "ds_sic = ds_sic.resample(time='QS-DEC').mean(dim=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 823
    },
    "id": "_HxVbPKseZWi",
    "outputId": "adc6ab04-7902-4ce2-f09b-e605e2966376"
   },
   "outputs": [],
   "source": [
    "# our data has now been converted to seasonal means:\n",
    "ds_sic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RtpwBp1vgYvG",
    "outputId": "01560f08-c6f2-4694-bdd6-5a94cb984635"
   },
   "outputs": [],
   "source": [
    "# we can select the winter data using where\n",
    "winter = ds_sic.where(ds_sic.time.dt.season == 'DJF',drop=True)\n",
    "# the other seasons are called 'MAM', 'JJA', 'SON', and we can\n",
    "# select them the same way\n",
    "\n",
    "# another way to do the same thing is by taking every 4th element\n",
    "# along the time dimension, like this:\n",
    "# winter = ds_sic.isel(time=np.arange(0,ds_sic.time.size,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 823
    },
    "id": "FaAkhKgnjUka",
    "outputId": "4f1d8aa5-765b-4e47-b5a3-4eab63dadd85"
   },
   "outputs": [],
   "source": [
    "# let's check the new data set:\n",
    "ds_sic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "H4mDZUZtgzL1",
    "outputId": "438a2f07-1905-44b8-c3c7-93716be889e2"
   },
   "outputs": [],
   "source": [
    "# and plot the time series at one of the points:\n",
    "winter.isel(x=150,y=20).siconc.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 823
    },
    "id": "V4Vnzlb0i6QE",
    "outputId": "d7aaf35f-ab01-41c2-99ce-e1c66f59905f"
   },
   "outputs": [],
   "source": [
    "# we can also make a 5-year average, to keep the overall shape\n",
    "# of the sea ice decline while reducing the data volume:\n",
    "winter_5y = winter.resample(time='5y').mean()\n",
    "winter_5y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "q4YmKqwgkCqj",
    "outputId": "73697847-3357-4818-c917-fb516f9b6ac7"
   },
   "outputs": [],
   "source": [
    "# we now have only 19 points in the time dimension. Let's look at the time series\n",
    "# at the same point as before to compare the form:\n",
    "winter_5y.isel(x=150,y=20).siconc.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTiKW1zHl2V8"
   },
   "source": [
    "## What next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "od-2JZqul5Fl"
   },
   "source": [
    "You should now be ready to apply a clustering analysis to the seasonal data. Start with either sea ice concentration or sea ice volume for a model and emissions scenario that you choose. How many clusters is a good choice? What do the average time series look like for each cluster? Do the clusters compare well with the geographical regions that are defined for the Arctic, or do those regions mix a number of different clusters according to your analysis? Once you have looked at one variable, you might check whether you get different results for the sea ice concentration and sea ice volume? Can you use both predictors, and how do your results change if you do?\n",
    "\n",
    "Once you have implemented the clustering for one simulation, you have a number of choices. You can do any one, or a combination, of the following:\n",
    "- analyse other ensemble members for the same model and scenario: these are simulations that are experiencing the same greenhouse gas changes and so differences from your chosen simulation will be due to \"natural\" climate variability as simulated by that model\n",
    "\n",
    "- analyse different models with the same scenario: this will allow you to see differences that are due to the way that particular models simulate the sea ice. It is a way to quantify uncertainty in the future state of the Arctic that is due to the differences between models.\n",
    "\n",
    "- analyse the same model in different emission scenarios: this will allow you to see how the ice loss spatial and temporal patterns might change as a function of greenhouse gas emissions.\n",
    "\n",
    "For whichever option you choose, you can check whether you obtain the same spatial patterns for your clusters and average time series for each cluster. You could also see whether or not your first model can be reused (using the ```.predict``` method) to obtain the same results on this new data set, or whether it is necessary to train a new model in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8mXAVpflttf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
